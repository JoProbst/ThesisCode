{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "# read csvs without header, column names are qid, sep, uuid, score\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "qrels = pt.io.read_qrels(\"../dataset/assessments/qrels.txt\") # type: ignore\n",
    "qcred = pt.io.read_qrels(\"../dataset/assessments/qcredibility.txt\") # type: ignore\n",
    "qread = pt.io.read_qrels(\"../dataset/assessments/qreadability.txt\") # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def load_topics(path):\n",
    "    with open(path) as f:\n",
    "        root = ET.fromstring(f.read())\n",
    "    topic_dict = {}\n",
    "    for topic in root.findall(\"topic\"):\n",
    "        topic_id = topic.findtext(\"id\")\n",
    "        topic_query = topic.findtext(\"query\")\n",
    "        if topic_id and topic_query:\n",
    "            topic_dict[topic_id] = topic_query.strip().lower()\n",
    "    topics = pd.DataFrame(topic_dict.items(), columns=[\"qid\", \"query\"]) \n",
    "    topics[\"query\"] = topics[\"query\"].str.replace(r'\\W+', ' ', regex=True)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_topics(\"../dataset/topics/topics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_passages = pd.read_csv(\"../dataset/Webdoc/data/txt_min_length_50.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "# rank documents with custom function that evaluates readability of the document\n",
    "def readability_score(text):\n",
    "    score = textstat.flesch_reading_ease(text)\n",
    "    return score \n",
    "\n",
    "def text_standard(text):\n",
    "    score = textstat.text_standard(text, float_output=True)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Cytokine dysregulation is a central driver of chronic inflammatory diseases such as multiple sclerosis (MS). Here, we sought to determine the characteristic cellular and cytokine polarization profile in patients with relapsing–remitting multiple sclerosis (RRMS) by high-dimensional single-cell mass cytometry (CyTOF). Using a combination of neural network-based representation learning algorithms, we identified an expanded T helper cell subset in patients with MS, characterized by the expression of granulocyte–macrophage colony-stimulating factor and the C-X-C chemokine receptor type 4. This cellular signature, which includes expression of very late antigen 4 in peripheral blood, was also enriched in the central nervous system of patients with relapsing–remitting multiple sclerosis. In independent validation cohorts, we confirmed that this cell population is increased in patients with MS compared with other inflammatory and non-inflammatory conditions. Lastly, we also found the population to be reduced under effective disease-modifying therapy, suggesting that the identified T cell profile represents a specific therapeutic target in MS.\"\n",
    "print(readability_score(sample))\n",
    "print(text_standard(sample))\n",
    "docid = \"00bd4ea6-90d6-40ac-93ab-d0be20d6c8e5\"\n",
    "print(readability_score(all_passages[all_passages.docid == docid].text.values[0]))\n",
    "print(text_standard(all_passages[all_passages.docid == docid].text.values[0]))\n",
    "print(all_passages[all_passages.docid == docid].text.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Tymoteusz/distilbert-base-uncased-kaggle-readability\", truncation=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Tymoteusz/distilbert-base-uncased-kaggle-readability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the transformer score \n",
    "def readability_score_transformer(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    return logits[0][0].item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability_score_transformer(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_scores_transformer = []\n",
    "for index, row in all_passages.iterrows():\n",
    "    read_scores_transformer.append(readability_score_transformer(row[\"text\"]))\n",
    "all_passages[\"readability_score_transformer\"] = read_scores_transformer\n",
    "\n",
    "all_passages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_passages[\"flesch_reading_ease\"] = all_passages[\"text\"].apply(readability_score)\n",
    "all_passages[\"text_standard\"] = all_passages[\"text\"].apply(text_standard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_passages[['docid', 'readability_score_transformer', 'flesch_reading_ease', 'text_standard']].to_csv('all_passages_readability_scores.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "MODEL = \"jy46604790/Fake-News-Bert-Detect\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "clf = pipeline(\"text-classification\", model=MODEL, tokenizer=MODEL, max_length=512, device=device)\n",
    "\n",
    "truth_label = \"LABEL_1\"\n",
    "def credibility_score(text, model=clf, truth_label=truth_label):\n",
    "    score = model(text)\n",
    "    if score[0][\"label\"] == truth_label:\n",
    "        return score[0][\"score\"]\n",
    "    else:\n",
    "        return 1 - score[0][\"score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_passages[\"credibility_score_bert\"] = all_passages[\"text\"].apply(credibility_score)\n",
    "# get texts as list from all_passages\n",
    "texts = all_passages[\"text\"].tolist()\n",
    "# get credibility scores for all texts\n",
    "credibility_scores = clf(texts, truncation=True, max_length=512, verbose=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get credibility scores from the list of dictionaries\n",
    "credibility_scores_float = [score[\"score\"] if score[\"label\"] == truth_label else 1 - score[\"score\"] for score in credibility_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add credibility scores to the dataframe\n",
    "all_passages[\"credibility_score_bert\"] = credibility_scores_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_passages.head()\n",
    "# save docid and credibility score to tsv\n",
    "all_passages[[\"docid\", \"credibility_score_bert\"]].to_csv(\"all_passages_credibility_scores_bert.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
